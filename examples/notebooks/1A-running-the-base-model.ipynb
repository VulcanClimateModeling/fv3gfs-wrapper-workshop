{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1A: The elements of the most basic run file\n",
    "\n",
    "In this notebook we will introduce how to set up an example run in a Jupyter notebook with ipyparallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursively copy the contents of the example run directory to a clean folder in the docker container\n",
    "\n",
    "We'll also override the input namelist of the reference run directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE = \"reference_rundir\"\n",
    "RUNDIR = \"rundir_1A\"\n",
    "\n",
    "if os.path.isdir(RUNDIR):\n",
    "    shutil.rmtree(RUNDIR)\n",
    "shutil.copytree(REFERENCE, RUNDIR);\n",
    "shutil.copy(\"input_1A.nml\", os.path.join(RUNDIR, \"input.nml\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the ipyparallel session for the notebook\n",
    "\n",
    "This involves running a couple shell commands (hence the `%%bash` cell magic command at the top of the following cell); note this means these commands are executed in the shell rather than the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# if you get a crash, add --debug to this command to put more info in logs\n",
    "# logs are in /root/.ipython/profile_mpi/log\n",
    "ipcluster start --profile=mpi -n 6 --daemonize\n",
    "sleep 10  # command is asynchronous, so let's wait to avoid an error in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client(profile='mpi', targets='all', block=True)\n",
    "dv = rc[:]\n",
    "dv.activate()\n",
    "dv.block = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running IPython Parallel on 6 MPI engines\n",
      "Commands in the following cells will be executed in parallel (disable with %autopx)\n",
      "%autopx enabled\n"
     ]
    }
   ],
   "source": [
    "print(\"Running IPython Parallel on {0} MPI engines\".format(len(rc.ids)))\n",
    "print(\"Commands in the following cells will be executed in parallel (disable with %autopx)\")\n",
    "%autopx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `mpi4py` gain access to the communicator for the notebook.\n",
    "\n",
    "Using the `size` and `rank` attributes of the communicator we show the number of MPI ranks we are running the notebook with, as well as the rank of the particular process.  There are different output streams for the different ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Number of ranks is 6.\n",
      "I am rank 0.\n",
      "[stdout:1] \n",
      "Number of ranks is 6.\n",
      "I am rank 1.\n",
      "[stdout:2] \n",
      "Number of ranks is 6.\n",
      "I am rank 2.\n",
      "[stdout:3] \n",
      "Number of ranks is 6.\n",
      "I am rank 3.\n",
      "[stdout:4] \n",
      "Number of ranks is 6.\n",
      "I am rank 4.\n",
      "[stdout:5] \n",
      "Number of ranks is 6.\n",
      "I am rank 5.\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "mpi_size = comm.size\n",
    "mpi_rank = comm.rank\n",
    "\n",
    "print(f\"Number of ranks is {mpi_size}.\")\n",
    "print(f\"I am rank {mpi_rank}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter the run directory\n",
    "\n",
    "Next we move into the run directory we created.  Note that we need to re-import `os` and re-define any variables we created before we started the cluster.  `fv3gfs.wrapper` requires that its routines are called from within a valid run directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] ['.DS_Store', 'INPUT', 'README.md', 'RESTART', 'aerosol.dat', 'co2historicaldata_2010.txt', 'co2historicaldata_2011.txt', 'co2historicaldata_2012.txt', 'co2historicaldata_2013.txt', 'co2historicaldata_2014.txt', 'co2historicaldata_2015.txt', 'co2historicaldata_2016.txt', 'data_table', 'diag_table', 'field_table', 'fv3.exe', 'fv3config.yml', 'global_o3prdlos.f77', 'grb', 'input.nml', 'sfc_emissivity_idx.txt', 'solarconstant_noaa_an.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "RUNDIR = \"rundir_1A\"\n",
    "os.chdir(RUNDIR)\n",
    "\n",
    "if mpi_rank == 0:\n",
    "    print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By listing the contents of the current directory on the zeroth rank, we can see what is needed in a run directory for FV3GFS.  This is everything that we would need to run the base fortran model.  There are a number of input files (e.g. `co2historicaldata_2*.txt`, the files in the `grb` directory, etc.) as well as some files used for configuring the model, e.g.:\n",
    "\n",
    "- `input.nml` the input namelist for the model (all the parameter settings for the simulation)\n",
    "- `diag_table` the specification of which diagnostics the model will output\n",
    "- `field_table` the specification of the tracers that the model will use for microphysics or other purposes\n",
    "\n",
    "`INPUT/` contains the initial conditions for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the model configured?\n",
    "\n",
    "The main place to look at this is in the namelist.  There is a handy Python tool for doing this called `f90nml`: https://f90nml.readthedocs.io/en/latest/.  For instance we can look at the coupler namelist to find information about the start date of the run (`current_date`), the model timestep (`dt_atmos`), and the amount of time the model is configured to run for (in this case `hours = 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Namelist([('atmos_nthreads', 1), ('calendar', 'julian'), ('current_date', [2016, 8, 1, 0, 0, 0]), ('days', 0), ('dt_atmos', 900), ('dt_ocean', 900), ('hours', 1), ('memuse_verbose', True), ('minutes', 0), ('months', 0), ('ncores_per_node', 32), ('seconds', 0), ('use_hyper_thread', True)])\n"
     ]
    }
   ],
   "source": [
    "import f90nml\n",
    "namelist = f90nml.read(\"input.nml\")\n",
    "\n",
    "if comm.rank == 0:\n",
    "    print(namelist[\"coupler_nml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can run the model\n",
    "\n",
    "We'll start by importing the `wrapper` and running the `initialize` method.  This allocates the necessary memory for the model, and loads in the intial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fv3gfs import wrapper\n",
    "wrapper.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to run the model for the amount of time specified in the namelist, we can write our own main loop in Python.  The wrapper contains a convenient method for computing the number of timesteps required to run for the length of time in the namelist, `wrapper.get_step_count` (in this case four steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 4\n"
     ]
    }
   ],
   "source": [
    "if comm.rank == 0:\n",
    "    print(wrapper.get_step_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic main loop requires just three steps:\n",
    "- `step_dynamics`\n",
    "- `step_physics`\n",
    "- `save_intermediate_restart_if_enabled`\n",
    "\n",
    "These three steps, executed in this order within the main loop, ensure bit-for-bit reproducibility with the pure fortran model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(wrapper.get_step_count()):\n",
    "    wrapper.step_dynamics()\n",
    "    wrapper.step_physics()\n",
    "    wrapper.save_intermediate_restart_if_enabled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when the loop is finished, we can call `wrapper.cleanup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: verify that the model ran by loading in diagnostics it produced\n",
    "\n",
    "Diagnostics from the fortran model are output in netCDF files.  As currently configured, this simulation outputs one netCDF file per output category per cubed sphere tile.  In this simulation some diagnostics are configured to be stored every timestep in the `atmos_dt_atmos.tile{tile}.nc` output files.  With your favorite Python library (e.g. `netcdf4-python`, `xarray`), load one or more of these files and plot a variable (using e.g., `matplotlib.pyplot.pcolormesh`) to verify that the model ran.\n",
    "\n",
    "We can disable the MPI parallelism for now.  Note this means that we are back in the root directory of the docker container again, so all file paths have to take this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%autopx disabled\n"
     ]
    }
   ],
   "source": [
    "%autopx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
